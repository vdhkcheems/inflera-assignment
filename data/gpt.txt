Title: Improving Language Understanding by Generative Pre-Training  
Authors: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever  
Published: 2018  
Abstract: This paper introduces a generative pre-training approach for language models that uses a large corpus of unlabeled text to learn general language representations. These representations can then be fine-tuned for specific NLP tasks like classification or question answering. Unlike traditional models, this approach uses a single Transformer decoder and trains it using a two-step process: unsupervised generative pre-training followed by supervised discriminative fine-tuning.  
Introduction: Most NLP models are trained from scratch for each task, which is data-inefficient and computationally expensive. This work explores a more general-purpose approach: first train a language model on a large corpus to capture general language patterns, and then fine-tune it on task-specific data. The architecture is based on the Transformer decoder and is trained to predict the next word in a sequence.  
Model Architecture (Method): The model uses a multi-layer Transformer decoder architecture. In the pre-training phase, it is trained as a left-to-right language model on the BooksCorpus dataset. During fine-tuning, task-specific input and output formats are constructed, and supervised learning is applied. No task-specific modifications are made to the architecture.  
Applications and Results: The model achieves strong results on multiple tasks including textual entailment, semantic similarity, and reading comprehension, using minimal task-specific architecture modifications. It demonstrates that generative pre-training followed by discriminative fine-tuning can be a powerful framework for NLP tasks.