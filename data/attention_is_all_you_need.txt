Title: Attention Is All You Need  
Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin  
Published: 2017
Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on machine translation tasks show that the Transformer outperforms the existing models on both quality and training cost. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, outperforming previous benchmarks.
Introduction: Recurrent neural networks have been the standard in sequence modeling tasks like machine translation, but they suffer from limited parallelization and long training times. While convolutional models offer some parallelism, they still rely on a fixed-size context. Attention mechanisms have proven to be effective at capturing dependencies regardless of distance. This paper introduces the Transformer model, which forgoes recurrence entirely and uses attention mechanisms as its core building block. The resulting model is simpler, faster to train, and achieves state-of-the-art results.
Model Architecture (Method): The Transformer architecture consists of an encoder and a decoder, each made up of a stack of layers. Each encoder layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder layers include an additional third sub-layer that performs multi-head attention over the encoder’s output. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Scaled dot-product attention computes attention weights using a query, key, and value matrix. Positional encodings are added to the input embeddings to give the model information about the position of tokens in the sequence, since the model contains no recurrence or convolution. The architecture enables significantly more parallelization and reduces the path length for gradient flow during training. This allows faster convergence and better performance on sequence transduction tasks.
Applications and Results: The Transformer is evaluated on English-German and English-French translation tasks. It achieves better BLEU scores than previous models and trains faster. It also demonstrates strong results when scaled to larger datasets and model sizes.