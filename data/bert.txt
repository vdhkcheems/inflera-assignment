Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova  
Published: 2018  
Abstract: BERT (Bidirectional Encoder Representations from Transformers) is a novel language representation model that is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows the model to learn deep language understanding. BERT is pre-trained on two unsupervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). It significantly outperforms existing models on a wide range of NLP tasks like question answering and natural language inference.  
Introduction: Traditional pre-trained language models are either unidirectional or shallowly concatenated. This limits their capability to fully leverage context from both directions. BERT addresses this limitation by using a deep bidirectional Transformer encoder, enabling the model to consider the full context of a word in a sentence. The model is trained on the BooksCorpus and English Wikipedia using the MLM and NSP objectives.  
Model Architecture (Method): BERT uses the Transformer encoder architecture with multi-head self-attention layers. The base model has 12 layers, 768 hidden units, and 12 attention heads. The pre-training tasks include Masked Language Modeling, where 15% of the input tokens are masked and the model predicts them, and Next Sentence Prediction, which trains the model to understand sentence relationships. After pre-training, BERT is fine-tuned with just one additional output layer to achieve state-of-the-art performance on various NLP benchmarks.  
Applications and Results: BERT achieves state-of-the-art results on eleven NLP tasks, including the GLUE benchmark, SQuAD v1.1, and SQuAD v2.0. It sets a new standard for transfer learning in NLP and demonstrates that deep bidirectional training can significantly improve language understanding.